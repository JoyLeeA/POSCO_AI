{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20200603 Wednesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, interleave_keys\n",
    "from torchtext.datasets import TranslationDataset\n",
    "import mosestokenizer\n",
    "import torch\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Author: WonKee Lee (POSTECH)\n",
    "# \"Neural Machine Translation by Jointly Learning to Align and Translate\" 논문의 model 재현 (Toy code)\n",
    "#  (https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html 를 참고하여 수정함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### torchtext #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "tokenizer_en = mosestokenizer.MosesTokenizer('en')\n",
    "tokenizer_de = mosestokenizer.MosesTokenizer('de')\n",
    "\n",
    "# Field: Tensor로 표현할 데이터의 타입, 처리 프로세스 등을 정의하는 객체\n",
    "src = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tokenizer_en, # 어떤식으로 tokenizer 할건지 결정\n",
    "            lower=True, # 소문자 처리\n",
    "            batch_first=True) # [Batch, length] if False [length, Batch]\n",
    "\n",
    "tgt = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            pad_token=PAD,\n",
    "            tokenize=tokenizer_de,\n",
    "            lower=True,\n",
    "            init_token=BOS,\n",
    "            eos_token=EOS,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_f = './escape.en-de.tok.5k' # 5k는 데이터 사이즈 의미\n",
    "\n",
    "# parallel data 각각 (en, de) 을 src Field 와 tgt Field에 정의된 형태로 처리.\n",
    "parallel_dataset = TranslationDataset(path=prefix_f, exts=('.en', '.de'), fields=[('src', src), ('tgt', tgt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.translation.TranslationDataset object at 0x7f5274c5a710>\n",
      "dict_items([('src', ['once', 'again', ',', 'we', 'are', 'ignoring', 'the', 'fundamental', 'problem', ',', 'which', 'is', 'that', 'as', 'long', 'as', 'there', 'is', 'no', 'democracy', 'in', 'belgrade', ',', 'there', 'will', 'be', 'no', 'solution', 'for', 'kosovo', ',', 'just', 'as', 'there', 'will', 'be', 'no', 'solution', 'for', 'the', 'entire', 'yugoslav', 'population', '.']), ('tgt', ['das', 'bedeutet', ',', 'einmal', 'mehr', 'die', 'augen', 'vor', 'dem', 'grundproblem', 'zu', 'verschließen', ',', 'daß', 'es', 'nämlich', ',', 'solange', 'es', 'keine', 'demokratie', 'in', 'belgrad', 'gibt', ',', 'auch', 'keine', 'lösung', 'für', 'den', 'kosovo', 'geben', 'wird', ',', 'ebenso', 'wenig', 'für', 'die', 'gesamte', 'bevölkerung', 'jugoslawiens', '.', 'das', 'muß', 'man', 'endlich', 'begreifen', '.'])])\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset) \n",
    "\n",
    "print(parallel_dataset.examples[0].__dict__.items()) # src 및 tgt 에 대한 samples 를 포함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'again', ',', 'we', 'are', 'ignoring', 'the', 'fundamental', 'problem', ',', 'which', 'is', 'that', 'as', 'long', 'as', 'there', 'is', 'no', 'democracy', 'in', 'belgrade', ',', 'there', 'will', 'be', 'no', 'solution', 'for', 'kosovo', ',', 'just', 'as', 'there', 'will', 'be', 'no', 'solution', 'for', 'the', 'entire', 'yugoslav', 'population', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[0].src) # 첫번째 src 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['das', 'bedeutet', ',', 'einmal', 'mehr', 'die', 'augen', 'vor', 'dem', 'grundproblem', 'zu', 'verschließen', ',', 'daß', 'es', 'nämlich', ',', 'solange', 'es', 'keine', 'demokratie', 'in', 'belgrad', 'gibt', ',', 'auch', 'keine', 'lösung', 'für', 'den', 'kosovo', 'geben', 'wird', ',', 'ebenso', 'wenig', 'für', 'die', 'gesamte', 'bevölkerung', 'jugoslawiens', '.', 'das', 'muß', 'man', 'endlich', 'begreifen', '.']\n"
     ]
    }
   ],
   "source": [
    "print(parallel_dataset.examples[0].tgt) # 첫번째 tgt 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 사전 구축 ########\n",
    "# src, tgt 필드에 사전 구축\n",
    "src.build_vocab(parallel_dataset, min_freq=5, max_size=15000)\n",
    "tgt.build_vocab(parallel_dataset, min_freq=5, max_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
      "\n",
      "     <unk> |   0\n",
      "     <pad> |   1\n",
      "       the |   2\n",
      "         , |   3\n",
      "         . |   4\n",
      "        of |   5\n",
      "        to |   6\n",
      "       and |   7\n",
      "        in |   8\n",
      "         a |   9\n",
      "       @-@ |  10\n",
      "        is |  11\n",
      "       for |  12\n",
      "         ) |  13\n",
      "         ( |  14\n",
      "      that |  15\n"
     ]
    }
   ],
   "source": [
    "# 사전 내용 \n",
    "print(src.vocab.__dict__.keys())\n",
    "print('')\n",
    "for i, (k, v) in enumerate(src.vocab.stoi.items()): # stoi: word와 index 페어 정보를 얻을 수 있음\n",
    "    print ('{:>10s} | {:>3d}'.format(k, v)) \n",
    "    if i == 15 : break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = parallel_dataset.split(split_ratio=0.95) # 0.95 = train / 0.05 = valid 데이터로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch iterator 생성.\n",
    "# iterator 를 반복할 때 마다, batch 크기의 (src, tgt) 쌍의 parallel data가 출력됨.\n",
    "BATCH_SIZE = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE, # 각각에 대한 iterator를 뽑아준다.\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)), # 정렬의 방식 정의 (src 와 tgt를 둘다 고려하여 패딩을 최소화)\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 는 Batch 객체를 출력해주며, \n",
    "# Batch.src / Batch.tgt 로 parallel data각각에 대해 접근가능.\n",
    "\n",
    "# 예시.\n",
    "Batch = next(iter(train_iterator)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  23,   11,   73, 1634,   15,   32,    0,    0,  188,   68,  143,    8,\n",
       "            2,    0,  422,    4,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1],\n",
       "        [  27,   49, 2261,   15,   23,   55,   37,    2, 1308, 2293,    5,  218,\n",
       "          133, 2749,    7,    0,    2, 2508,   19,    0,    6,    0,    0,  113,\n",
       "            4],\n",
       "        [  52,   33,    6,   26,   62,    3,   87,    8,   15,    2, 2841,    0,\n",
       "           11,    0,   24,    2,  751,  193,   10,   62,    4,    1,    1,    1,\n",
       "            1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src 에 저장된 데이터 출력\n",
    "# Field에 정의된 형식으로 데이터 전처리 (indexing 포함.)\n",
    "# 가장 긴 문장을 기준으로, 그 보다 짧은 문장은 Padding idx(=1) 을 부여.\n",
    "Batch.src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   31,   15,  366,    0,    4,   57,   44,    0,    4,   57,  264,\n",
       "            0,    0, 1199,  269,   41,    5,    3,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1],\n",
       "        [   2,   31,   15,    0,    4,   35,   31,   34,    6,    0, 1631, 1768,\n",
       "          146,    4,    6,   17,    0,   26,    0,    0,    8,    0,   62,    4,\n",
       "            0,  159,    0,    5,    3],\n",
       "        [   2,   77,   18,   30,   75,    4,   69,   70,    4,   35,   37,    7,\n",
       "            0, 1208,   45,   29,    0,  174,    9,   75,  277,    5,    3,    1,\n",
       "            1,    1,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Field에 정의된 형식으로 데이터 전처리 (indexing + bos + eos 토큰 처리 됨.)\n",
    "Batch.tgt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "######                      Network 정의                               ########\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder 정의.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, src_ntoken: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.src_ntoken = src_ntoken\n",
    "\n",
    "        self.embedding = nn.Embedding(src_ntoken, hidden_dim, ## 사전의 크기\n",
    "                                      padding_idx=src.vocab.stoi['<pad>'])\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, # first hidden_dim: 입력, second: 출력\n",
    "                          bidirectional = True, batch_first=True) # batch_first = [B, L, dim]\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) # bidirectional hidden을 하나의 hidden size로 변환\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src)) # src idx 들이 embedding으로 바뀐다.\n",
    "                                                     # src = (Batch, Length), shape = (Batch, Length, hidden_dim)\n",
    "        # outputs: [B, L, D*2], hidden: [2, B, D] D = dimention, 2 = bidirectional 을 의미\n",
    "        # Note: if bidirectional=False then [B, L, D], [1, B, D]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        last_hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) # [B, D]\n",
    "        hidden = torch.tanh(last_hidden) # last bidirectional hidden\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention 모듈 정의 ###\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        attn_in = (enc_hid_dim * 2) + dec_hid_dim # bidirectional hidden + dec_hidden\n",
    "        self.linear = nn.Linear(attn_in, attn_dim)\n",
    "        self.merge = nn.Linear(attn_dim, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n",
    "        # decoder_hidden = (Batch, 1, Dim)\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        repeated_decoder_hidden = decoder_hidden.repeat(1, src_len, 1) # [B, src_len, D]\n",
    "\n",
    "        # enc의 각 step의 hidden + decoder의 hidden 의 결과값 # [B, src_len, D*2] --> [B, src_len, D]\n",
    "        # tanh(W*h_dec  + U*h_enc) 수식 부분.\n",
    "        energy = torch.tanh(self.linear(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2))) \n",
    "\n",
    "        score = self.merge(energy).squeeze(-1) # [B, src_len] 각 src 단어에 대한 점수 -> V^T tanh(W*h_dec  + U*h_enc) 부분\n",
    "        normalized_score = F.softmax(score, dim=1)  # softmax를 통해 확률분포값으로 변환\n",
    "        return  normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decoder 모듈 정의 ####\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, dec_ntoken: int, dropout: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(enc_hid_dim=hidden_dim,\n",
    "                                   dec_hid_dim=hidden_dim,\n",
    "                                   attn_dim=hidden_dim) # attn module, init 실행\n",
    "        \n",
    "        \n",
    "        self.dec_ntoken = dec_ntoken # vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(dec_ntoken, hidden_dim, padding_idx=tgt.vocab.stoi['<pad>']) # bidirection이 아님\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(self.hidden_dim*3, dec_ntoken)\n",
    "        self.sm = nn.LogSoftmax(dim=-1) # softmax activation\n",
    "\n",
    "    def _context_rep(self, dec_out: Tensor, enc_outs: Tensor) -> Tensor:\n",
    "\n",
    "        scores = self.attention(dec_out, enc_outs) # [B, L]\n",
    "        scores = scores.unsqueeze(1) # [B, 1, src_len] -> weight value (softmax)\n",
    "\n",
    "        # scores: (batch, 1, src_len),  ecn_outs: (Batch, src_len, dim)\n",
    "        context_vector = torch.bmm(scores, enc_outs) # weighted average -> (batch, 1, dec_dim): encoder의 각 hidden의 weighted sum\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        dec_outs = []\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input)) # (Batch, length, Dim)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0) # hidden = (Batch, Dim) --> (1, Batch, Dim)\n",
    "        # (Batch,1, dim) (batch, 1 ,dim)...\n",
    "        for emb_t in embedded.split(1, dim=1): # Batch의 각 time step (=각 단어) 에 대한 embedding 출력 \n",
    "            rnn_out, decoder_hidden = self.rnn(emb_t, decoder_hidden) # feed input with previous decoder hidden at each step\n",
    "\n",
    "            context = self._context_rep(rnn_out, encoder_outputs)\n",
    "            rnn_context = self.dropout(torch.cat([rnn_out, context], dim=2))\n",
    "            dec_out = self.linear(rnn_context)\n",
    "            dec_outs += [self.sm(dec_out)]\n",
    "\n",
    "        dec_outs = dec_outs[:-1] # trg = trg[:-1] # <E> 는 Decoder 입력으로 고려하지 않음.\n",
    "        dec_outs = torch.cat(dec_outs, dim=1) # convert list to tensor: [Batch, Length, Vocab_size]\n",
    "        return dec_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seq-to-Seq 모델 정의 ###\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor) -> Tensor:\n",
    "        encoder_outputs, hidden = self.encoder(src) # encoder_outputs = (Batch, length, Dim *2), hidden = (Batch, Dim)\n",
    "        dec_out = self.decoder(trg, hidden, encoder_outputs)\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src.vocab)  # src 사전 크기\n",
    "OUTPUT_DIM = len(tgt.vocab) # tgt 사전 크기\n",
    "HID_DIM = 64 # rnn, embedding, 등. 모든 hidden 크기를 해당 값으로 통일함. (실습의 용이성을 위함.)\n",
    "D_OUT = 0.3 # Dropout  확률\n",
    "\n",
    "# 상단의 예시에서 작은 Batch로 보여줬기 때문에, \n",
    "# Batch 크기를 원래대로 바꿔 iterator 다시 선언함.\n",
    "BATCH_SIZE = 15\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size=BATCH_SIZE,\n",
    "                                                    sort_key=lambda x: interleave_keys(len(x.src), len(x.tgt)),\n",
    "                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 및 디코더 생성\n",
    "# Seq2Seq 모델 생성\n",
    "encoder = Encoder(HID_DIM, INPUT_DIM, D_OUT)\n",
    "decoder = Decoder(HID_DIM, OUTPUT_DIM, D_OUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights) # 모델 파라미터 초기화\n",
    "optimizer = optim.Adam(model.parameters()) # Optimizer 설정\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt.vocab.stoi['<pad>']) # LOSS 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(2893, 64, padding_idx=1)\n",
      "    (rnn): GRU(64, 64, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (linear): Linear(in_features=192, out_features=64, bias=True)\n",
      "      (merge): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(2384, 64, padding_idx=1)\n",
      "    (rnn): GRU(64, 64, batch_first=True)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (linear): Linear(in_features=192, out_features=2384, bias=True)\n",
      "    (sm): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "The model has 893,393 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 모델 정보 및 파라미터 수 출력\n",
    "def count_parameters(model: nn.Module):\n",
    "    print(model)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 학습 함수 ###\n",
    "def train(model: nn.Module, iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer, criterion: nn.Module, clip: float):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "\n",
    "        optimizer.zero_grad() # gradient 제거\n",
    "\n",
    "        output = model(src, tgt) # [batch, length, vocab_size]\n",
    "        output = output.view(-1, output.size(-1)) # flatten --> (batch * length, vocab_size)\n",
    "\n",
    "        tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # remove <S> placed at first from targets\n",
    "        tgt = tgt.view(-1) # flatten target with shape = (batch * length)\n",
    "\n",
    "        loss = criterion(output, tgt) # tgt 이 내부적으로 one_hot으로 변환됨 --> (batch * length, vocab_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if(((i+1) % int(len(iterator)*0.2)) == 0):\n",
    "            num_complete = batch.batch_size * (i+1)\n",
    "            total_size = batch.batch_size * int(len(iterator))\n",
    "            ratio = num_complete/total_size * 100\n",
    "            print('| Current Epoch:  {:>4d} / {:<5d} ({:2d}%) | Train Loss: {:3.3f}'.\n",
    "                  format(num_complete, batch.batch_size * int(len(iterator)), round(ratio), loss.item())\n",
    "                  )\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 평가 함수 ###\n",
    "def evaluate(model: nn.Module, iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.tgt\n",
    "\n",
    "            output = model(src, tgt)\n",
    "            output = output.view(-1, output.size(-1)) # flatten (batch * length, vocab_size)\n",
    "\n",
    "            tgt = tgt.unsqueeze(-1)[:,1:,:].squeeze(-1).contiguous() # remove <S> placed at first from targets\n",
    "            tgt = tgt.view(-1) # flatten target with shape = (batch * length)\n",
    "            loss = criterion(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시간 카운트를 위한 기타 함수 #\n",
    "def epoch_time(start_time: int, end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 5.062\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 5.118\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 4.605\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 4.658\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 4.748\n",
      "=================================================================\n",
      "Epoch: 01 | Time: 0m 55s\n",
      "\tTrain Loss: 5.174 | Train PPL: 176.572\n",
      "\t Val. Loss: 4.825 |  Val. PPL: 124.567\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 4.596\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 4.642\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 4.904\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 4.492\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 4.278\n",
      "=================================================================\n",
      "Epoch: 02 | Time: 0m 55s\n",
      "\tTrain Loss: 4.653 | Train PPL: 104.884\n",
      "\t Val. Loss: 4.513 |  Val. PPL:  91.235\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 4.228\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 4.183\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 4.618\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 4.598\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 3.892\n",
      "=================================================================\n",
      "Epoch: 03 | Time: 0m 55s\n",
      "\tTrain Loss: 4.417 | Train PPL:  82.818\n",
      "\t Val. Loss: 4.332 |  Val. PPL:  76.063\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 4.603\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 3.941\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 3.897\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 4.002\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 4.432\n",
      "=================================================================\n",
      "Epoch: 04 | Time: 0m 48s\n",
      "\tTrain Loss: 4.245 | Train PPL:  69.743\n",
      "\t Val. Loss: 4.217 |  Val. PPL:  67.829\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 4.284\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 4.310\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 4.154\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 3.968\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 3.596\n",
      "=================================================================\n",
      "Epoch: 05 | Time: 0m 43s\n",
      "\tTrain Loss: 4.109 | Train PPL:  60.912\n",
      "\t Val. Loss: 4.126 |  Val. PPL:  61.906\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 3.805\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 4.158\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 3.711\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 4.295\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 3.964\n",
      "=================================================================\n",
      "Epoch: 06 | Time: 0m 43s\n",
      "\tTrain Loss: 4.010 | Train PPL:  55.138\n",
      "\t Val. Loss: 4.028 |  Val. PPL:  56.149\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 3.678\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 3.868\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 4.224\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 4.260\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 3.938\n",
      "=================================================================\n",
      "Epoch: 07 | Time: 0m 52s\n",
      "\tTrain Loss: 3.925 | Train PPL:  50.656\n",
      "\t Val. Loss: 3.997 |  Val. PPL:  54.442\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 3.918\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 4.040\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 4.114\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 3.737\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 3.740\n",
      "=================================================================\n",
      "Epoch: 08 | Time: 0m 54s\n",
      "\tTrain Loss: 3.857 | Train PPL:  47.334\n",
      "\t Val. Loss: 3.965 |  Val. PPL:  52.739\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 3.607\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 3.382\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 3.922\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 3.771\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 4.025\n",
      "=================================================================\n",
      "Epoch: 09 | Time: 0m 54s\n",
      "\tTrain Loss: 3.795 | Train PPL:  44.494\n",
      "\t Val. Loss: 3.932 |  Val. PPL:  51.001\n",
      "=================================================================\n",
      "| Current Epoch:   945 / 4755  (20%) | Train Loss: 4.171\n",
      "| Current Epoch:  1890 / 4755  (40%) | Train Loss: 3.554\n",
      "| Current Epoch:  2835 / 4755  (60%) | Train Loss: 3.401\n",
      "| Current Epoch:  3780 / 4755  (79%) | Train Loss: 3.626\n",
      "| Current Epoch:  4725 / 4755  (99%) | Train Loss: 3.648\n",
      "=================================================================\n",
      "Epoch: 10 | Time: 0m 55s\n",
      "\tTrain Loss: 3.746 | Train PPL:  42.339\n",
      "\t Val. Loss: 3.907 |  Val. PPL:  49.753\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10 # 최대 epoch 크기\n",
    "CLIP = 0.5 # weight cliping \n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print('='*65)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print('='*65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
